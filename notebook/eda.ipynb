{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the src directory\n",
    "sys.path.append(os.path.abspath('../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from data_and_preprocessing import load_data, preprocess_data\n",
    "from text_analysis import analyze_sentiment, extract_keywords\n",
    "from time_series_analysis import publication_frequency\n",
    "from publishers_analysis import publisher_activity, unique_domains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "df = load_data('G:/Tnxs/Kifiya3/Week1/Tnxs-kifiya3-week1-repo/src/data/raw_analyst_ratings.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['date'].head())\n",
    "\n",
    "# # Check for unique values or types\n",
    "# print(df['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Clean the date column\n",
    "# df['date'] = df['date'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to parse dates with multiple formats\n",
    "# def parse_dates(date_str):\n",
    "#     for fmt in (\"%Y-%m-%d %H:%M:%S\", \"%Y/%m/%d\", \"%d-%m-%Y\"):\n",
    "#         try:\n",
    "#             return pd.to_datetime(date_str, format=fmt)\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "#     return pd.NaT\n",
    "\n",
    "# # Apply the parsing function\n",
    "# df['date'] = df['date'].apply(parse_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check conversion results\n",
    "# print(df['date'].isna().sum(), \"dates could not be parsed.\")\n",
    "# print(df['date'].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55987 dates could not be parsed.\n",
      "0           NaT\n",
      "1           NaT\n",
      "2           NaT\n",
      "3           NaT\n",
      "4           NaT\n",
      "5           NaT\n",
      "6           NaT\n",
      "7           NaT\n",
      "8           NaT\n",
      "9           NaT\n",
      "10   2020-05-22\n",
      "11   2020-05-22\n",
      "12   2020-05-21\n",
      "13   2020-05-21\n",
      "14   2020-05-21\n",
      "15   2020-05-21\n",
      "16   2020-05-18\n",
      "17   2020-05-16\n",
      "18   2020-05-15\n",
      "19   2020-05-08\n",
      "20   2020-05-05\n",
      "21   2020-05-01\n",
      "22   2020-04-28\n",
      "23   2020-04-23\n",
      "24   2020-04-22\n",
      "25   2020-04-14\n",
      "26   2020-04-08\n",
      "27   2020-04-06\n",
      "28   2020-04-02\n",
      "29   2020-04-01\n",
      "30   2020-03-31\n",
      "31   2020-03-30\n",
      "32   2020-03-30\n",
      "33   2020-03-27\n",
      "34   2020-03-26\n",
      "35   2020-03-26\n",
      "36   2020-03-26\n",
      "37   2020-03-20\n",
      "38   2020-03-16\n",
      "39   2020-03-16\n",
      "40   2020-03-13\n",
      "41   2020-03-12\n",
      "42   2020-03-11\n",
      "43   2020-03-03\n",
      "44   2020-03-02\n",
      "45   2020-02-24\n",
      "46   2020-02-24\n",
      "47   2020-02-24\n",
      "48   2020-02-19\n",
      "49   2020-02-18\n",
      "Name: date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Check conversion results\n",
    "print(df['date'].isna().sum(), \"dates could not be parsed.\")\n",
    "print(df['date'].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0                           date  headline_length\n",
      "count  1.407328e+06                        1351341     1.407328e+06\n",
      "mean   7.072454e+05  2015-07-06 06:39:44.879020288     7.312051e+01\n",
      "min    0.000000e+00            2009-02-14 00:00:00     3.000000e+00\n",
      "25%    3.538128e+05            2012-12-04 00:00:00     4.700000e+01\n",
      "50%    7.072395e+05            2015-07-21 00:00:00     6.400000e+01\n",
      "75%    1.060710e+06            2018-03-09 00:00:00     8.700000e+01\n",
      "max    1.413848e+06            2020-06-03 00:00:00     5.120000e+02\n",
      "std    4.081009e+05                            NaN     4.073531e+01\n",
      "Number of articles per publisher: publisher\n",
      "Paul Quintaro        228373\n",
      "Lisa Levin           186979\n",
      "Benzinga Newsdesk    150484\n",
      "Charles Gross         96732\n",
      "Monica Gerson         82380\n",
      "                      ...  \n",
      "MoneyGeek                 1\n",
      "muathe                    1\n",
      "Robert Morris             1\n",
      "LeftCoastHedgie           1\n",
      "Jeremie Capron            1\n",
      "Name: count, Length: 1034, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Descriptive Statistics\n",
    "print(df.describe())\n",
    "print(\"Number of articles per publisher:\", publisher_activity(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.11 TiB for an array with shape (1407328, 108488) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Text Analysis\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m analyze_sentiment(df)\n\u001b[1;32m----> 3\u001b[0m keywords, keyword_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Tnxs\\Kifiya3\\Week1\\Tnxs-kifiya3-week1-repo\\src\\text_analysis.py:16\u001b[0m, in \u001b[0;36mextract_keywords\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     14\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out(), \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Tnxs\\Kifiya3\\Week1\\Tnxs-kifiya3-week1-repo\\venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1181\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1180\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1181\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mg:\\Tnxs\\Kifiya3\\Week1\\Tnxs-kifiya3-week1-repo\\venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1301\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.11 TiB for an array with shape (1407328, 108488) and data type int64"
     ]
    }
   ],
   "source": [
    "\n",
    "# Text Analysis\n",
    "df = analyze_sentiment(df)\n",
    "keywords, keyword_matrix = extract_keywords(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time Series Analysis\n",
    "freq = publication_frequency(df)\n",
    "\n",
    "# Publisher Analysis\n",
    "print(\"Unique domains:\", unique_domains(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
