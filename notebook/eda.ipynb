{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl-M3o4gp6ey",
        "outputId": "5c022325-1587-4acb-c6f1-a3f38022a9f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/beltobel/Tnxs-kifiya3-week1-repo.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNV1Ey1Krdl4",
        "outputId": "e7b38848-db8f-4ec0-e04e-1f5e55254ba2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Tnxs-kifiya3-week1-repo' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('Tnxs-kifiya3-week1-repo')"
      ],
      "metadata": {
        "id": "ys5bmx-XsOno"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_iisrUosWpO",
        "outputId": "109d5768-4acc-423d-93c6-cdb34fce7281"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "notebook  README.md  requirements.txt  scripts\tsrc  Tnxs-kifiya3-week1-repo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IG0N-mltWYu",
        "outputId": "d63df3d9-e15b-4ab6-f4fc-411ffbdf6ea2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Tnxs-kifiya3-week1-repo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "205rkQEftFA2",
        "outputId": "809a222d-cc93-4217-c0a2-ecba598fd0e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Tnxs-kifiya3-week1-repo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkszSINip2TD",
        "outputId": "bdc38fe1-8f02-41b0-b46f-c1792ccd2be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "notebook  README.md  requirements.txt  scripts\tsrc  Tnxs-kifiya3-week1-repo\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl5xC0DKtstG",
        "outputId": "3dc99e28-6e89-437f-c59f-6d0c2c8a36cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mTnxs-kifiya3-week1-repo/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout task-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vP8nFBgtzUQ",
        "outputId": "744fcd9f-c6ab-47be-8c39-8d979b3033b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'task-1' set up to track remote branch 'task-1' from 'origin'.\n",
            "Switched to a new branch 'task-1'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout task-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gxTTXZ0t_bA",
        "outputId": "52939599-b89f-4ab4-d4bc-10d120aa4d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'task-1'\n",
            "Your branch is up to date with 'origin/task-1'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"beletebogale2@gmail.com\"\n",
        "!git config --global user.name \"beltobel\"\n"
      ],
      "metadata": {
        "id": "5LE22haKuoQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "aBAzg0esuMTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m\"new change from colab\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ_lNAjWuRLl",
        "outputId": "e8c62aaf-b9d1-48e8-f237-9a7b5539ec45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch task-1\n",
            "Your branch is up to date with 'origin/task-1'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D8ObhmBvFKJ",
        "outputId": "e6afce1d-a2ba-4e67-a032-b178be598c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "tIrrV7amuHb4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_awdYnHCp2TG"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# import os\n",
        "\n",
        "# Add the path to the src directory\n",
        "# sys.path.append(os.path.abspath('../src'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xnwJOSmyp2TG"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from src.data_and_preprocessing import load_data, preprocess_data\n",
        "# from src.text_analysis import analyze_sentiment, extract_keywords\n",
        "from src.time_series_analysis import publication_frequency\n",
        "from src.publishers_analysis import publisher_activity, unique_domains\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HZMtZ5Vd49ls",
        "outputId": "440980b4-3d15-4b2b-c5e9-a60fd1ea26d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "id": "hmuBc_ny5STE",
        "outputId": "a239da75-fe5e-418a-9573-a20cd47ba798",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "d_cXhxqDp2TH"
      },
      "outputs": [],
      "source": [
        "df = load_data('../../content/drive/My Drive/kifiya3_training/raw_analyst_ratings.csv')  # Adjust the path as needed\n",
        "# Load and preprocess data\n",
        "# df = load_data('raw_analyst_ratings.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IXUgMqhRp2TJ"
      },
      "outputs": [],
      "source": [
        "df = preprocess_data(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlgPYhHwp2TJ",
        "outputId": "8712d965-e8aa-4bad-ce94-f82c216a455d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1351341 dates could not be parsed.\n",
            "0    2020-06-05 10:30:54-04:00\n",
            "1    2020-06-03 10:45:20-04:00\n",
            "2    2020-05-26 04:30:07-04:00\n",
            "3    2020-05-22 12:45:06-04:00\n",
            "4    2020-05-22 11:38:59-04:00\n",
            "5    2020-05-22 11:23:25-04:00\n",
            "6    2020-05-22 09:36:20-04:00\n",
            "7    2020-05-22 09:07:04-04:00\n",
            "8    2020-05-22 08:37:59-04:00\n",
            "9    2020-05-22 08:06:17-04:00\n",
            "10                         NaT\n",
            "11                         NaT\n",
            "12                         NaT\n",
            "13                         NaT\n",
            "14                         NaT\n",
            "15                         NaT\n",
            "16                         NaT\n",
            "17                         NaT\n",
            "18                         NaT\n",
            "19                         NaT\n",
            "Name: date, dtype: datetime64[ns, UTC-04:00]\n"
          ]
        }
      ],
      "source": [
        "# Check conversion results\n",
        "print(df['date'].isna().sum(), \"dates could not be parsed.\")\n",
        "print(df['date'].head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJmM9poBp2TK",
        "outputId": "05384716-3fbc-4b53-a0c6-4f8c112c0930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Unnamed: 0  headline_length\n",
            "count  1.407328e+06     1.407328e+06\n",
            "mean   7.072454e+05     7.312051e+01\n",
            "std    4.081009e+05     4.073531e+01\n",
            "min    0.000000e+00     3.000000e+00\n",
            "25%    3.538128e+05     4.700000e+01\n",
            "50%    7.072395e+05     6.400000e+01\n",
            "75%    1.060710e+06     8.700000e+01\n",
            "max    1.413848e+06     5.120000e+02\n",
            "Number of articles per publisher: publisher\n",
            "Paul Quintaro                      228373\n",
            "Lisa Levin                         186979\n",
            "Benzinga Newsdesk                  150484\n",
            "Charles Gross                       96732\n",
            "Monica Gerson                       82380\n",
            "                                    ...  \n",
            "Shazir Mucklai - Imperium Group         1\n",
            "Laura Jennings                          1\n",
            "Eric Martin                             1\n",
            "Jose Rodrigo                            1\n",
            "Jeremie Capron                          1\n",
            "Name: count, Length: 1034, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Descriptive Statistics\n",
        "print(df.describe())\n",
        "print(\"Number of articles per publisher:\", publisher_activity(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5XktKNmp2TL",
        "outputId": "90aa2aec-fbb6-49a4-f40f-5879a3905809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique domains: domain\n",
            "N/A                       1399240\n",
            "benzinga.com                 7937\n",
            "gmail.com                     139\n",
            "andyswan.com                    5\n",
            "investdiva.com                  2\n",
            "tothetick.com                   2\n",
            "eosdetroit.io                   1\n",
            "forextraininggroup.com          1\n",
            "stockmetrix.net                 1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Time Series Analysis\n",
        "freq = publisher_activity(df)\n",
        "\n",
        "# Publisher Analysis\n",
        "print(\"Unique domains:\", unique_domains(df))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bea49bwkK9jT",
        "outputId": "f0c9e6ee-610e-473e-faf6-5eb242f7e667"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HG6uW9qsp2TL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b4b966-10c1-45ae-b9a3-0a8052bdee4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                headline  \\\n",
            "0                Stocks That Hit 52-Week Highs On Friday   \n",
            "1             Stocks That Hit 52-Week Highs On Wednesday   \n",
            "2                          71 Biggest Movers From Friday   \n",
            "3           46 Stocks Moving In Friday's Mid-Day Session   \n",
            "4      B of A Securities Maintains Neutral on Agilent...   \n",
            "...                                                  ...   \n",
            "99995  Intuit, ASML Holding, and STMicroelectronics A...   \n",
            "99996                      ING Group Opens ASML With Buy   \n",
            "99997  Benzinga's Top Pre-Market NASDAQ Losers (ARMH,...   \n",
            "99998  Benzinga's Top Pre-Market NASDAQ Gainers (ERIC...   \n",
            "99999  Benzinga's Top Pre-Market NASDAQ Losers (OVTI,...   \n",
            "\n",
            "                                       preprocessed_text  \\\n",
            "0                                stocks hit highs friday   \n",
            "1                             stocks hit highs wednesday   \n",
            "2                                  biggest movers friday   \n",
            "3                                  stocks moving session   \n",
            "4      b securities maintains neutral agilent raises ...   \n",
            "...                                                  ...   \n",
            "99995  asml stmicroelectronics attending ubs tech con...   \n",
            "99996                           ing group opens asml buy   \n",
            "99997                                  top nasdaq losers   \n",
            "99998                                 top nasdaq gainers   \n",
            "99999                                  top nasdaq losers   \n",
            "\n",
            "                                               sentiment sentiment_label  \n",
            "0      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...         neutral  \n",
            "1      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...         neutral  \n",
            "2      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...         neutral  \n",
            "3      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...         neutral  \n",
            "4      {'neg': 0.0, 'neu': 0.732, 'pos': 0.268, 'comp...        positive  \n",
            "...                                                  ...             ...  \n",
            "99995  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...         neutral  \n",
            "99996  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...         neutral  \n",
            "99997  {'neg': 0.548, 'neu': 0.161, 'pos': 0.29, 'com...        negative  \n",
            "99998  {'neg': 0.0, 'neu': 0.526, 'pos': 0.474, 'comp...        positive  \n",
            "99999  {'neg': 0.548, 'neu': 0.161, 'pos': 0.29, 'com...        negative  \n",
            "\n",
            "[100000 rows x 4 columns]\n",
            "\n",
            "Keywords: ['aa' 'aaa' 'aaaai' ... 'zyskind' 'æterna' 'östergrens']\n",
            "\n",
            "Keyword Matrix:\n",
            " [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the input text: tokenize using split, remove stopwords, and lower case.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = text.lower().split()  # Tokenize by splitting on spaces and lowercasing\n",
        "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Remove stopwords and non-alphabetic words\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "def analyze_sentiment(df, column_name):\n",
        "    \"\"\"\n",
        "    Perform sentiment analysis on a specific column of a DataFrame.\n",
        "    \"\"\"\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    # Apply preprocessing and sentiment analysis\n",
        "    df['preprocessed_text'] = df[column_name].apply(preprocess_text)\n",
        "    df['sentiment'] = df['preprocessed_text'].apply(lambda x: sia.polarity_scores(x))\n",
        "    df['sentiment_label'] = df['sentiment'].apply(lambda x: 'positive' if x['compound'] > 0.05 else ('negative' if x['compound'] < -0.05 else 'neutral'))\n",
        "    return df\n",
        "\n",
        "def extract_keywords(df, column_name):\n",
        "    \"\"\"\n",
        "    Extract keywords from a specific column of a DataFrame using CountVectorizer.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    preprocessed_texts = df[column_name].apply(preprocess_text)\n",
        "    X = vectorizer.fit_transform(preprocessed_texts)\n",
        "    return vectorizer.get_feature_names_out(), X.toarray()\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your dataset\n",
        "    file_path = '../../content/drive/My Drive/kifiya3_training/raw_analyst_ratings.csv'\n",
        "\n",
        "    # Load a portion of the data to reduce size for large datasets\n",
        "    nrows = 100000  # Adjust this number to control the number of rows\n",
        "    df = pd.read_csv(file_path, nrows=nrows)\n",
        "\n",
        "    # Specify the column to process\n",
        "    column_name = 'headline'  # Replace with the appropriate column name from your dataset\n",
        "\n",
        "    # Perform sentiment analysis\n",
        "    df = analyze_sentiment(df, column_name)\n",
        "\n",
        "    # Extract keywords\n",
        "    keywords, keyword_matrix = extract_keywords(df, column_name)\n",
        "\n",
        "\n",
        "    # Print results\n",
        "    print(df[['headline', 'preprocessed_text', 'sentiment', 'sentiment_label']])\n",
        "    print(\"\\nKeywords:\", keywords)\n",
        "    print(\"\\nKeyword Matrix:\\n\", keyword_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = df['sentiment_label'].value_counts()\n",
        "print(\"\\nSentiment Counts:\")\n",
        "print(sentiment_counts)"
      ],
      "metadata": {
        "id": "gXqkWptD0ZXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce52c90b-f72a-4683-f5cd-043a1dc03aac"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentiment Counts:\n",
            "sentiment_label\n",
            "neutral     56380\n",
            "positive    29288\n",
            "negative    14332\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L_-Cck80S_A8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}